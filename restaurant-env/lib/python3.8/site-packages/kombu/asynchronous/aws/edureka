Rolls are used to assign permition to other aws services
policy are use to give acess to user permition(we can create policy to a roll and roll can create a permition to asw services)
region->zone->pop
It = compute,storage,networking,app interation,managing tool
CDN = edge location(cash),local zone = pop(compute,storage)
SDK = programaticaly use aws service(python)

Fedarated user:- managed outside AWS,some other acc user using my account(create provition to that user,to use my accountng ,using roll we can do this(sts))

best practice
IAM Access analyzer

without internet we can transfer data using Direct connect location(this is aws internali give high speed low latency network for data tranzfer)


CDN  to incresce speed of tranzfer ,cash is use that is edge location


edge location only limited to cash but, local zone are   pop have computation,server,storage etc

t2 micro is free instence EC2

security group(every thing is denaied , execpt allowed IP)


setatefull:- inbound rules  n outbound  rules,it will remember what we give as input n give same as output
stateless:-only insound,it will forgot what we give in input,

linux = ssh
apache = http

security group is state 


putti;- ssh client ,download putti,this is client ,mechin running and putti is ssh client, require public ip


can change instence with same family
(t2c means t2)


we  can downscale or upscale in same family.


along with EC2 there will be storage also,they cost different



Haduup = EMR (Big data)


vertualise compute on top of aws = EC2

server to webserver(apache,httpt)


ami = ec2 mechin
volume = snapshot

snapshot cast money

no automated snapshot ec2

snapshot is backup mechanisam

volume = snapshot = instance


key pair will be 1 for 100 of ec2

types of storage
a. block storage(while createing ec2)(require os)
b. objects store (s3)(pendrive or google drive)
c. elastic block storage (EBS) (hard drive can attach 1 at a time,1 time i instance can attach to ebs)(DEDICATE STORE)
d. elastic file system(EFS) different developer keep data to let access other developer(network)
e.instence store n EBS 2 TYPOE OF BLOCK STORE
FSX()

glasiar:-  old historical data 

gateway facility to migrate data to 


SNOW BALL = BREAFCASE TO STORE DATA


host means server

create ec2 instence,not host(Its created by aws)

NIC :NETWORK 


in 1 phisical server

talk to eack other in aws using private ip

what ever reach internet that use public ip address

vpn:vertual private network(involve internet so use public ip )

1 to 1 instence elastic ip

1 account 5 elastic ip
we cannot attahe 


private is static ,
public is variing,to make public to static we use elasic ip .



3 tenence is only for ec2 aws.

website is created,user are using public ip and if i want to do some update then after update ip will change,if we use elastic ip ,then it wont change


ondemond costly,reserved mid cost,spot cheep


upfront = advance money 

snapshot means backup
create snapshot is for ami


create ec2 create block storage automaticaly

block storage:- harddisk,what ever store on it it require os,

pendrive sore data but it need os to view data,this is block storage

EBS = block storagetorage = EFS(ELASTIC FILE SYSTEM)can attach 1 instence at a time,can detached n attache


in EFS ,,DIFFERENT DEVOLUPER DO WORK n keep in file,so that other developer can use

object store(s3):-  Like drop box,store it as object ex:pendrive

special effect move,:- use FSx luster(file system with added capacity)

Glaciear:old data are store in it

file system, shared drives ,network s
snapshot is for volume 





getMascoteColorByProtectionStatus



s3 created in us reagon,i am in india,if i want to add some thing then it may cause delay,solution is accelarate upload()



multi tenancy:- phisical data server share by many user

backup disappear in 30 days,but snap shot will retain



What is the best way to protect a file in Amazon S3 against accidental delete?
Enable versioning on the S3 bucket, this option ask for user as can we delete,if user want to delete s3,then he have to come to bucket console n reset this option


file = 1 terabite limit




s3 vertion, avoid accedent delete 

to roll back to previus vertion,we should delete current vertion


only use of bucket,charge,not for vertioning


ec2 cannot be attached to s3


choice to select sse-s3 encription and sse-kms

expiration policy ,set s3 to expire after some days of creation is done in lifecycle policy




hadupe file system

stored in structer and non structure

kasandra = amazon keyspace(db)

amazone aurora= mysql,postgres(rds:relation database serviceSS

)

redshift = rdbms



stand by is very similar to master db,but repica is delay in data copy


replica use for read only,but not stand by

if i stop db instence then also it will charge
1. time of run db
2. stored pergb/month
3. read write opreration charge
4. storage backup charged




ec2 = many ebs
1 ebs = 1 ec2


data migration = kinesis(use in ott platform)


cash are very fast,data lost when instence is delewted,cannot take snapshot,instence store = cash

instence store = default,attached,cannot attched


ecr and ebs should be in same reagon



S3 IS CHEEP THEN EFS AND EBS

GOOGLE DRIVE ,DROPBOX = S3

database can use along with EC2

backup is stored in s3,maintain by aws

can created as snapshot, using snaposhot can create new db


while creating AMI we need to keep in mind what os need to select


platform specificSSS


backup will delete when we delete db,but snapshot will remain,by usind snapshot we cacreate new db

dynamodb :- record is json(each row),its fast.go on increate data aws increse capacity.


RCU:- 100 read per sec
WCU:- write per sec
mysql create db,create table



more rcu increse billing
more wcu increse billing

this is use in mobile gaming.

max doc size 4kb.

dynamo db , cluster of db

primary key = partition key,sort key

IP address:- 10.88.0.0/24

16,8,4,2,0
cider= ip talk to eack other



1st loadbalanser = classic loadbalancer
network loadbalancer
application loadbalancer


distribute load,balance trafic b/w ec2 instences


gateway loadbalancer

redshift never write to s3
redshift store data in it's cluster



big data = redshift


redshift,sit data in s3 and process it in redshift.

by etl lod data to redshift then process




if we want to process data in s3 itself then ,use specrum


keyspace is managed casandra


